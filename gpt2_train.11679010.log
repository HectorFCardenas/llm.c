Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_*.bin            |
| val data pattern      | dev/data/fineweb10B/fineweb_val_*.bin              |
| output log dir        | log124M                                            |
| checkpoint_every      | 5000                                               |
| resume                | 0                                                  |
| micro batch size B    | 32                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 524288                                             |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 6.000000e-04                                       |
| warmup iterations     | 700                                                |
| final LR fraction     | 0.000000e+00                                       |
| weight decay          | 1.000000e-01                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | -1                                                 |
| val_loss_every        | 250                                                |
| val_max_steps         | 20                                                 |
| sample_every          | 20000                                              |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | enabled                                            |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA A100-PCIE-40GB                              |
| peak TFlops           | 312.0                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | d12                                                |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 19560                                              |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | yes                                                |
+-----------------------+----------------------------------------------------+
| num_processes         | 1                                                  |
| zero_stage            | 1                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocated 237 MiB for model parameters
batch_size B=32 * seq_len T=1024 * num_processes=1 and total_batch_size=524288
=> setting grad_accum_steps=16
---
WARNING: Failed to open the tokenizer file gpt2_tokenizer.bin
The Tokenizer is a new feature added April 14 2024.
Re-run `python train_gpt2.py` to write it
---
allocating 237 MiB for parameter gradients
allocating 19806 MiB for activations
allocating 474 MiB for AdamW optimizer state m
allocating 474 MiB for AdamW optimizer state v
allocating 474 MiB for master copy of params
device memory usage: 22198 MiB / 40338 MiB
memory per sequence: 618 MiB
 -> estimated maximum batch size: 61
val loss nan
step    1/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.57e-07 | 3994.72 ms | 33.8% bf16 MFU | 131245 tok/s
step    2/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.71e-06 | 3694.99 ms | 36.5% bf16 MFU | 141892 tok/s
step    3/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.57e-06 | 3706.88 ms | 36.4% bf16 MFU | 141658 tok/s
step    4/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.43e-06 | 3568.69 ms | 37.8% bf16 MFU | 143500 tok/s
step    5/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.29e-06 | 3559.82 ms | 37.9% bf16 MFU | 144519 tok/s
step    6/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.14e-06 | 3696.34 ms | 36.5% bf16 MFU | 143927 tok/s
step    7/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.00e-06 | 3556.83 ms | 38.0% bf16 MFU | 144583 tok/s
step    8/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.86e-06 | 3613.69 ms | 37.4% bf16 MFU | 144666 tok/s
step    9/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.71e-06 | 3561.16 ms | 37.9% bf16 MFU | 145046 tok/s
step   10/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.57e-06 | 3555.93 ms | 38.0% bf16 MFU | 145370 tok/s
step   11/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 9.43e-06 | 3575.71 ms | 37.8% bf16 MFU | 145526 tok/s
step   12/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.03e-05 | 3573.85 ms | 37.8% bf16 MFU | 145662 tok/s
step   13/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.11e-05 | 3570.51 ms | 37.8% bf16 MFU | 145790 tok/s
step   14/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.20e-05 | 3574.62 ms | 37.8% bf16 MFU | 145881 tok/s
step   15/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.29e-05 | 3572.93 ms | 37.8% bf16 MFU | 145964 tok/s
step   16/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.37e-05 | 3644.13 ms | 37.1% bf16 MFU | 145769 tok/s
step   17/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.46e-05 | 3584.49 ms | 37.7% bf16 MFU | 145814 tok/s
step   18/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.54e-05 | 3657.33 ms | 36.9% bf16 MFU | 145602 tok/s
step   19/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.63e-05 | 3598.25 ms | 37.5% bf16 MFU | 145611 tok/s
step   20/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.71e-05 | 3597.62 ms | 37.5% bf16 MFU | 145621 tok/s
step   21/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.80e-05 | 3600.58 ms | 37.5% bf16 MFU | 145620 tok/s
step   22/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.89e-05 | 3598.53 ms | 37.5% bf16 MFU | 145626 tok/s
step   23/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 1.97e-05 | 3598.69 ms | 37.5% bf16 MFU | 145630 tok/s
step   24/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.06e-05 | 3593.94 ms | 37.6% bf16 MFU | 145648 tok/s
step   25/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.14e-05 | 3593.89 ms | 37.6% bf16 MFU | 145665 tok/s
step   26/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.23e-05 | 3601.72 ms | 37.5% bf16 MFU | 145658 tok/s
step   27/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.31e-05 | 3601.50 ms | 37.5% bf16 MFU | 145652 tok/s
step   28/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.40e-05 | 3605.38 ms | 37.4% bf16 MFU | 145637 tok/s
step   29/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.49e-05 | 3600.39 ms | 37.5% bf16 MFU | 145636 tok/s
step   30/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.57e-05 | 3609.35 ms | 37.4% bf16 MFU | 145611 tok/s
step   31/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.66e-05 | 3604.68 ms | 37.5% bf16 MFU | 145601 tok/s
step   32/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.74e-05 | 3607.05 ms | 37.4% bf16 MFU | 145585 tok/s
step   33/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.83e-05 | 3605.44 ms | 37.4% bf16 MFU | 145575 tok/s
step   34/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 2.91e-05 | 3606.05 ms | 37.4% bf16 MFU | 145563 tok/s
step   35/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.00e-05 | 3621.31 ms | 37.3% bf16 MFU | 145516 tok/s
step   36/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.09e-05 | 3610.24 ms | 37.4% bf16 MFU | 145498 tok/s
step   37/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.17e-05 | 3602.93 ms | 37.5% bf16 MFU | 145499 tok/s
step   38/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.26e-05 | 3605.46 ms | 37.4% bf16 MFU | 145494 tok/s
step   39/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.34e-05 | 3606.20 ms | 37.4% bf16 MFU | 145488 tok/s
step   40/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.43e-05 | 3610.50 ms | 37.4% bf16 MFU | 145472 tok/s
step   41/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.51e-05 | 3605.59 ms | 37.4% bf16 MFU | 145468 tok/s
step   42/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.60e-05 | 3608.75 ms | 37.4% bf16 MFU | 145458 tok/s
step   43/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.69e-05 | 3606.20 ms | 37.4% bf16 MFU | 145454 tok/s
step   44/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.77e-05 | 3609.73 ms | 37.4% bf16 MFU | 145442 tok/s
step   45/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.86e-05 | 3607.00 ms | 37.4% bf16 MFU | 145437 tok/s
step   46/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 3.94e-05 | 3614.55 ms | 37.4% bf16 MFU | 145415 tok/s
step   47/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.03e-05 | 3607.41 ms | 37.4% bf16 MFU | 145411 tok/s
step   48/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.11e-05 | 3603.73 ms | 37.5% bf16 MFU | 145415 tok/s
step   49/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.20e-05 | 3611.17 ms | 37.4% bf16 MFU | 145403 tok/s
step   50/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.29e-05 | 3606.90 ms | 37.4% bf16 MFU | 145400 tok/s
step   51/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.37e-05 | 3612.23 ms | 37.4% bf16 MFU | 145386 tok/s
step   52/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.46e-05 | 3609.79 ms | 37.4% bf16 MFU | 145378 tok/s
step   53/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.54e-05 | 3612.99 ms | 37.4% bf16 MFU | 145364 tok/s
step   54/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.63e-05 | 3608.85 ms | 37.4% bf16 MFU | 145359 tok/s
step   55/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.71e-05 | 3613.47 ms | 37.4% bf16 MFU | 145345 tok/s
step   56/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.80e-05 | 3608.25 ms | 37.4% bf16 MFU | 145343 tok/s
step   57/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.89e-05 | 3606.02 ms | 37.4% bf16 MFU | 145345 tok/s
step   58/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 4.97e-05 | 3606.03 ms | 37.4% bf16 MFU | 145348 tok/s
step   59/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.06e-05 | 3603.32 ms | 37.5% bf16 MFU | 145356 tok/s
step   60/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.14e-05 | 3615.47 ms | 37.3% bf16 MFU | 145338 tok/s
step   61/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.23e-05 | 3616.86 ms | 37.3% bf16 MFU | 145318 tok/s
step   62/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.31e-05 | 3616.94 ms | 37.3% bf16 MFU | 145299 tok/s
step   63/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.40e-05 | 3625.10 ms | 37.2% bf16 MFU | 145264 tok/s
step   64/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.49e-05 | 3617.88 ms | 37.3% bf16 MFU | 145246 tok/s
step   65/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.57e-05 | 3614.09 ms | 37.4% bf16 MFU | 145237 tok/s
step   66/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.66e-05 | 3613.72 ms | 37.4% bf16 MFU | 145229 tok/s
step   67/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.74e-05 | 3612.35 ms | 37.4% bf16 MFU | 145224 tok/s
step   68/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.83e-05 | 3615.30 ms | 37.3% bf16 MFU | 145213 tok/s
step   69/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 5.91e-05 | 3613.20 ms | 37.4% bf16 MFU | 145208 tok/s
step   70/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.00e-05 | 3605.21 ms | 37.5% bf16 MFU | 145219 tok/s
step   71/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.09e-05 | 3616.29 ms | 37.3% bf16 MFU | 145206 tok/s
step   72/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.17e-05 | 3821.63 ms | 35.3% bf16 MFU | 144795 tok/s
step   73/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.26e-05 | 3612.53 ms | 37.4% bf16 MFU | 144812 tok/s
step   74/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.34e-05 | 3614.68 ms | 37.4% bf16 MFU | 144824 tok/s
step   75/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.43e-05 | 3614.22 ms | 37.4% bf16 MFU | 144836 tok/s
step   76/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.51e-05 | 3649.69 ms | 37.0% bf16 MFU | 144776 tok/s
step   77/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.60e-05 | 3612.85 ms | 37.4% bf16 MFU | 144793 tok/s
step   78/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.69e-05 | 3616.20 ms | 37.3% bf16 MFU | 144803 tok/s
step   79/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.77e-05 | 3609.99 ms | 37.4% bf16 MFU | 144825 tok/s
step   80/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.86e-05 | 3650.78 ms | 37.0% bf16 MFU | 144763 tok/s
step   81/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 6.94e-05 | 3627.39 ms | 37.2% bf16 MFU | 144751 tok/s
step   82/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.03e-05 | 3618.95 ms | 37.3% bf16 MFU | 144758 tok/s
step   83/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.11e-05 | 3624.21 ms | 37.3% bf16 MFU | 144753 tok/s
step   84/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.20e-05 | 3617.67 ms | 37.3% bf16 MFU | 144761 tok/s
step   85/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.29e-05 | 3622.99 ms | 37.3% bf16 MFU | 144759 tok/s
step   86/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.37e-05 | 3615.84 ms | 37.3% bf16 MFU | 144771 tok/s
step   87/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.46e-05 | 3609.83 ms | 37.4% bf16 MFU | 144795 tok/s
step   88/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.54e-05 | 3607.89 ms | 37.4% bf16 MFU | 144821 tok/s
step   89/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.63e-05 | 3620.91 ms | 37.3% bf16 MFU | 144820 tok/s
step   90/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.71e-05 | 3620.20 ms | 37.3% bf16 MFU | 144820 tok/s
step   91/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.80e-05 | 3613.11 ms | 37.4% bf16 MFU | 144834 tok/s
step   92/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.89e-05 | 3616.39 ms | 37.3% bf16 MFU | 144841 tok/s
step   93/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 7.97e-05 | 3625.11 ms | 37.2% bf16 MFU | 144831 tok/s
step   94/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.06e-05 | 3620.13 ms | 37.3% bf16 MFU | 144830 tok/s
step   95/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.14e-05 | 3616.25 ms | 37.3% bf16 MFU | 144838 tok/s
step   96/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.23e-05 | 3617.07 ms | 37.3% bf16 MFU | 144844 tok/s
step   97/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.31e-05 | 3613.42 ms | 37.4% bf16 MFU | 144856 tok/s
step   98/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.40e-05 | 3621.02 ms | 37.3% bf16 MFU | 144853 tok/s
step   99/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.49e-05 | 3613.67 ms | 37.4% bf16 MFU | 144865 tok/s
step  100/19560 | loss     nan (+nanz)| norm    nan (+nanz)| lr 8.57e-05 | 3616.slurmstepd: error: *** JOB 11679010 ON g002 CANCELLED AT 2024-10-10T18:59:58 DUE TO TIME LIMIT ***
